{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Git Repositories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Index git repo\n",
    "1) Clone git repo\n",
    "2) split files to chunks\n",
    "3) create embedding from each chunk\n",
    "4) save all embeddings and chunks in np.array+list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Iterable\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import openai\n",
    "import tiktoken\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "OPENAI_KEY = \n",
    "CHATGPT_MODEL = \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_python_file(file_path, tokenizer, max_len=1000):\n",
    "    \"\"\"create chunks of texts that end with \\n\\n and are not longer than max_len.\n",
    "    In case a chunk is longer than max_len, it will NOT be splitted into multiple chunks.\n",
    "    TODO YONIGO: maybe split python file with:\n",
    "\n",
    "        ast.parse(file_path.read())\n",
    "        for child_node in ast.iter_child_nodes(tree):\n",
    "            print(ast.unparse(child_node))\n",
    "\n",
    "    So that each chunk will be a python node. problem is ast removes comments and docstrings.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        document = file.read()\n",
    "    sep_t = tokenizer.encode(\"\\n\\n\")[0]\n",
    "    doc_t = tokenizer.encode(document)\n",
    "    end_indices = [i for i, c in enumerate(doc_t) if c == sep_t]\n",
    "    if len(end_indices) == 0:\n",
    "        end_indices = [len(doc_t) - 1]\n",
    "    current_start = 0\n",
    "    current_end = end_indices[0]\n",
    "\n",
    "    for i in end_indices[1:]:\n",
    "        if i - current_start <= max_len:\n",
    "            current_end = i\n",
    "        else:\n",
    "            # if current_end - current_start > max_len:\n",
    "            #     print(f\"WARNING {file_path} has a chunk longer than {max_len} tokens: {current_end-current_start}\")\n",
    "            yield tokenizer.decode(doc_t[current_start : current_end + 1])\n",
    "            current_start = current_end + 1\n",
    "            current_end = i\n",
    "    if current_end - current_start > 0:\n",
    "        yield tokenizer.decode(doc_t[current_start : current_end + 1])\n",
    "\n",
    "\n",
    "def iterate_repo_chunks(git_repo: str, tokenizer, chunk_size=1000):\n",
    "    \"\"\"iterate over all python files in a git repo and yield chunks of text.\n",
    "    TODO YONIGO: make exclude_dirs a list of regexes.\n",
    "    \"\"\"\n",
    "    with TemporaryDirectory() as tmpdir:\n",
    "        os.system(f\"git clone {git_repo} {tmpdir}\")\n",
    "        for python_file in Path(tmpdir).glob(\"**/*.py\"):\n",
    "            if python_file.is_dir():\n",
    "                continue\n",
    "            if python_file.name.startswith(\"test_\"):\n",
    "                continue\n",
    "            yield from (\n",
    "                f\"file name: {python_file.relative_to(tmpdir)}\\nfile content:\\n{chunk}\"\n",
    "                for chunk in chunk_python_file(python_file, tokenizer, chunk_size)\n",
    "            )\n",
    "\n",
    "\n",
    "def batch_generator(iterable: Iterable, batch_size: int):\n",
    "    \"\"\"Yield successive batch_size chunks from the iterable.\n",
    "    openai embeedings api gets batches of chunks.\n",
    "    \"\"\"\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == batch_size:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    if current_batch:\n",
    "        yield current_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB:\n",
    "    def __init__(self, texts: list[str], embeddings: list[npt.NDArray]) -> None:\n",
    "        self.texts = texts\n",
    "        # Yes the vector db is a np.ndarray. Using Pinecone/ANN would be better for ##MUTCH## larger datasets.\n",
    "        self.vectors = np.stack(embeddings)\n",
    "\n",
    "    def search(self, vector: npt.NDArray[np.float32], k: int = 5, min_similarity=0.0) -> list[str]:\n",
    "        \"\"\"Search for k nearest documents to the given vector with cosine similarity\"\"\"\n",
    "        scores = np.dot(self.vectors, vector) / (np.linalg.norm(self.vectors, axis=1) * np.linalg.norm(vector))\n",
    "        indices = np.argpartition(scores, -k)[-k:]\n",
    "        return [self.texts[i] for i in indices if scores[i] > min_similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/var/folders/2x/bfct2nqs5h54sr249_przxy00000gn/T/tmpgsx4_q0j'...\n"
     ]
    }
   ],
   "source": [
    "def index_repo(repo_path: str, openai_key: str, chunk_size: int = 1000) -> VectorDB:\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    EMBEDDING_BATCH_SIZE = 500\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    for text_batch in batch_generator(iterate_repo_chunks(repo_path, tokenizer, chunk_size), EMBEDDING_BATCH_SIZE):\n",
    "        # TODO YONIGO: can be run concurrently with acreate\n",
    "        response = openai.Embedding.create(model=EMBEDDING_MODEL, input=text_batch, api_key=openai_key)\n",
    "        embeddings_batch = [np.array(e[\"embedding\"]) for e in response[\"data\"]]\n",
    "        texts.extend(text_batch)\n",
    "        embeddings.extend(embeddings_batch)\n",
    "        # documents.extend([Document(text, embedding) for text, embedding in zip(text_batch, embeddings_batch)])\n",
    "\n",
    "    db = VectorDB(texts, embeddings)\n",
    "    return db\n",
    "\n",
    "\n",
    "db = index_repo(\"https://github.com/tiangolo/fastapi.git\", OPENAI_KEY, chunk_size=1000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival \n",
    "For any given `question` I will use `topk_chunks` to retrieve the most relevant chunks.\n",
    "The most relevant chunks are chunks with highest cosine similarity between the question embedding and the chunk embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_chunks(question: str, db: VectorDB, openai_key: str, k: int = 5, min_similarity=0.0) -> list[str]:\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=[question], api_key=openai_key)\n",
    "    question_embedding = np.array(response[\"data\"][0][\"embedding\"])\n",
    "    return db.search(question_embedding, k=k, min_similarity=min_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file name: fastapi/routing.py\\nfile content:\\n\\nasync def serialize_response(\\n    *,\\n    field: Optional[ModelField] = None,\\n    response_content: Any,\\n    include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\\n    exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\\n    by_alias: bool = True,\\n    exclude_unset: bool = False,\\n    exclude_defaults: bool = False,\\n    exclude_none: bool = False,\\n    is_coroutine: bool = True,\\n) -> Any:\\n    if field:\\n        errors = []\\n        response_content = _prepare_response_content(\\n            response_content,\\n            exclude_unset=exclude_unset,\\n            exclude_defaults=exclude_defaults,\\n            exclude_none=exclude_none,\\n        )\\n        if is_coroutine:\\n            value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\\n        else:\\n            value, errors_ = await run_in_threadpool(\\n                field.validate, response_content, {}, loc=(\"response\",)\\n            )\\n        if isinstance(errors_, ErrorWrapper):\\n            errors.append(errors_)\\n        elif isinstance(errors_, list):\\n            errors.extend(errors_)\\n        if errors:\\n            raise ValidationError(errors, field.type_)\\n        return jsonable_encoder(\\n            value,\\n            include=include,\\n            exclude=exclude,\\n            by_alias=by_alias,\\n            exclude_unset=exclude_unset,\\n            exclude_defaults=exclude_defaults,\\n            exclude_none=exclude_none,\\n        )\\n    else:\\n        return jsonable_encoder(response_content)\\n\\n\\nasync def run_endpoint_function(\\n    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool\\n) -> Any:\\n    # Only called by get_request_handler. Has been split into its own function to\\n    # facilitate profiling endpoints, since inner functions are harder to profile.\\n    assert dependant.call is not None, \"dependant.call must be a function\"\\n\\n    if is_coroutine:\\n        return await dependant.call(**values)\\n    else:\\n        return await run_in_threadpool(dependant.call, **values)\\n\\n',\n",
       " 'file name: docs_src/additional_responses/tutorial001.py\\nfile content:\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import JSONResponse\\nfrom pydantic import BaseModel\\n\\n\\nclass Item(BaseModel):\\n    id: str\\n    value: str\\n\\n\\nclass Message(BaseModel):\\n    message: str\\n\\n\\napp = FastAPI()\\n\\n',\n",
       " 'file name: docs_src/response_directly/tutorial001.py\\nfile content:\\nfrom datetime import datetime\\nfrom typing import Union\\n\\nfrom fastapi import FastAPI\\nfrom fastapi.encoders import jsonable_encoder\\nfrom fastapi.responses import JSONResponse\\nfrom pydantic import BaseModel\\n\\n\\nclass Item(BaseModel):\\n    title: str\\n    timestamp: datetime\\n    description: Union[str, None] = None\\n\\n\\napp = FastAPI()\\n\\n',\n",
       " 'file name: fastapi/responses.py\\nfile content:\\nfrom typing import Any\\n\\nfrom starlette.responses import FileResponse as FileResponse  # noqa\\nfrom starlette.responses import HTMLResponse as HTMLResponse  # noqa\\nfrom starlette.responses import JSONResponse as JSONResponse  # noqa\\nfrom starlette.responses import PlainTextResponse as PlainTextResponse  # noqa\\nfrom starlette.responses import RedirectResponse as RedirectResponse  # noqa\\nfrom starlette.responses import Response as Response  # noqa\\nfrom starlette.responses import StreamingResponse as StreamingResponse  # noqa\\n\\ntry:\\n    import ujson\\nexcept ImportError:  # pragma: nocover\\n    ujson = None  # type: ignore\\n\\n\\ntry:\\n    import orjson\\nexcept ImportError:  # pragma: nocover\\n    orjson = None  # type: ignore\\n\\n\\nclass UJSONResponse(JSONResponse):\\n    def render(self, content: Any) -> bytes:\\n        assert ujson is not None, \"ujson must be installed to use UJSONResponse\"\\n        return ujson.dumps(content, ensure_ascii=False).encode(\"utf-8\")\\n\\n\\nclass ORJSONResponse(JSONResponse):\\n    media_type = \"application/json\"\\n\\n',\n",
       " 'file name: docs_src/custom_response/tutorial009c.py\\nfile content:\\nfrom typing import Any\\n\\nimport orjson\\nfrom fastapi import FastAPI, Response\\n\\napp = FastAPI()\\n\\n\\nclass CustomORJSONResponse(Response):\\n    media_type = \"application/json\"\\n\\n    def render(self, content: Any) -> bytes:\\n        assert orjson is not None, \"orjson must be installed\"\\n        return orjson.dumps(content, option=orjson.OPT_INDENT_2)\\n\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = topk_chunks(\"how are responses serialized?\", db, OPENAI_KEY, k=5, min_similarity=0.0)\n",
    "topk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "steps:\n",
    "1) get topk chunks for the question\n",
    "2) send all chunks to chatgpt with the question\n",
    "3) send all the answers to gpt again to reduce the answers to one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_prompt = \"\"\"\n",
    "    Use the following portion of a python codebase to try to answer the question. \n",
    "    python code:\\n\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    ANSWER:\n",
    "\"\"\"\n",
    "reduce_prompt = \"\"\"\n",
    "    These answers are based on different portions of the codebase.\n",
    "    combine them to get a better coherent single answer to the question.\n",
    "    question: {question}\n",
    "    list of answers:\\n\n",
    "    {answers}\\n\n",
    "    SINGLE ANSWER:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_chunk(question, chunk, openai_key) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about a python git repository\"},\n",
    "        {\"role\": \"user\", \"content\": aq_prompt.format(context=chunk, question=question)},\n",
    "    ]\n",
    "\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        messages=messages,\n",
    "        model=CHATGPT_MODEL,\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        api_key=OPENAI_KEY,\n",
    "    )\n",
    "    current_answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return current_answer\n",
    "\n",
    "\n",
    "async def reduce_answers(question, answers, openai_key) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about a python git repository\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": reduce_prompt.format(question=question, answers=\"\\n\".join([f\"answer:{a}\\n\" for a in answers])),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        messages=messages,\n",
    "        model=CHATGPT_MODEL,\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        api_key=OPENAI_KEY,\n",
    "    )\n",
    "    current_answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return current_answer\n",
    "\n",
    "\n",
    "async def map_reduce(question: str, topk_chunks: list[str], openai_key: str) -> str:\n",
    "    chunk_answers = await asyncio.gather(*[answer_chunk(question, chunk, openai_key) for chunk in topk_chunks])\n",
    "    return await reduce_answers(question, chunk_answers, openai_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses in the python git repository are serialized using JSON format and the `jsonable_encoder` function from the `fastapi.encoders` module is used to convert the response data to a JSON serializable format. The `JSONResponse` class from the `fastapi.responses` module is then used to create a response object with the serialized data. Additionally, there are two custom response classes, `UJSONResponse` and `ORJSONResponse`, which use the `ujson` and `orjson` libraries respectively to serialize the response content. The serialization is done using the `render` method of the response class. The `CustomORJSONResponse` class's `render` method also uses the `orjson` library to serialize the response content into a JSON-formatted byte string.\n"
     ]
    }
   ],
   "source": [
    "answer = await map_reduce(\"how are responses serialized?\", topk, OPENAI_KEY)\n",
    "print(answer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end2end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/var/folders/2x/bfct2nqs5h54sr249_przxy00000gn/T/tmp5pjttvzz'...\n"
     ]
    }
   ],
   "source": [
    "db = index_repo(\"https://github.com/tiangolo/fastapi.git\", OPENAI_KEY, chunk_size=1000)\n",
    "question = \"how are responses serialized?\"\n",
    "topk = topk_chunks(question, db, OPENAI_KEY, k=5, min_similarity=0.0)\n",
    "answer = await map_reduce(question, topk, OPENAI_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses in the python git repository are serialized using various methods. The `serialize_response` function defined in the `fastapi/routing.py` file is used to serialize responses by taking in various parameters such as the response content, fields to include or exclude, and whether to exclude unset, default, or None values. The `jsonable_encoder` function from the `fastapi.encoders` module is also used to convert the response data to a JSON serializable format. Additionally, the `JSONResponse` class from the `fastapi.responses` module is used to create a response object with the serialized data. The `CustomORJSONResponse` class's `render` method uses the `orjson` library to serialize the response content into bytes. There are also two custom response classes, `UJSONResponse` and `ORJSONResponse`, which use the `ujson` and `orjson` libraries respectively to serialize responses.\n"
     ]
    }
   ],
   "source": [
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitgpt-YVaQ39eL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
